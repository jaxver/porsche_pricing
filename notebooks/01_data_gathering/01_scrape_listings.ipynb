{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config import DATA_DIR_BRONZE, SCRAPING_CONFIG, ALL_LISTING_URLS_FILE\n",
    "from elferspot_listings.utils.helpers import setup_logging, save_data\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(level='INFO')\n",
    "logger.info(\"Data Gathering Notebook initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbaa89",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up scraping parameters and user agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping configuration\n",
    "BASE_URL = SCRAPING_CONFIG['base_url']\n",
    "USER_AGENT = SCRAPING_CONFIG['user_agent']\n",
    "TIMEOUT = SCRAPING_CONFIG['request_timeout']\n",
    "DELAY = SCRAPING_CONFIG['delay_between_requests']\n",
    "\n",
    "print(f\"Base URL: {BASE_URL}\")\n",
    "print(f\"Timeout: {TIMEOUT}s\")\n",
    "print(f\"Delay between requests: {DELAY}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943edeb",
   "metadata": {},
   "source": [
    "## Step 1: Load or Define Listing URLs\n",
    "\n",
    "Load existing listing URLs or define scraping targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3491d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load URLs from file if it exists\n",
    "if ALL_LISTING_URLS_FILE.exists():\n",
    "    urls_df = pd.read_csv(ALL_LISTING_URLS_FILE)\n",
    "    listing_urls = urls_df['URL'].tolist()\n",
    "    logger.info(f\"Loaded {len(listing_urls)} URLs from {ALL_LISTING_URLS_FILE}\")\n",
    "else:\n",
    "    # Define initial URLs to scrape\n",
    "    listing_urls = [\n",
    "        f\"{BASE_URL}/en/porsche-911-for-sale\",\n",
    "        f\"{BASE_URL}/en/porsche-boxster-for-sale\",\n",
    "        f\"{BASE_URL}/en/porsche-cayman-for-sale\",\n",
    "        # Add more URLs as needed\n",
    "    ]\n",
    "    logger.info(f\"Using {len(listing_urls)} predefined URLs\")\n",
    "\n",
    "print(f\"Total URLs to process: {len(listing_urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ce0c5",
   "metadata": {},
   "source": [
    "## Step 2: Scraping Functions\n",
    "\n",
    "Define functions to fetch and parse listing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str, headers: dict = None) -> str:\n",
    "    \"\"\"Fetch HTML content from URL.\"\"\"\n",
    "    if headers is None:\n",
    "        headers = {'User-Agent': USER_AGENT}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_listing(html: str, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse listing HTML and extract relevant data.\n",
    "    \n",
    "    Returns dictionary with listing details.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # This is a template - adjust selectors based on actual website structure\n",
    "    listing_data = {\n",
    "        'URL': url,\n",
    "        'Scraped_At': datetime.now().isoformat(),\n",
    "        'Title': None,\n",
    "        'Model': None,\n",
    "        'Series': None,\n",
    "        'Year of construction': None,\n",
    "        'Mileage': None,\n",
    "        'price': None,\n",
    "        'currency': None,\n",
    "        'Transmission': None,\n",
    "        'Drive': None,\n",
    "        'Exterior color': None,\n",
    "        'Interior color': None,\n",
    "        'Condition': None,\n",
    "        'Car location': None,\n",
    "        'Matching numbers': None,\n",
    "        'Number of vehicle owners': None,\n",
    "        'Paint-to-Sample (PTS)': None,\n",
    "        'Ready to drive': None,\n",
    "    }\n",
    "    \n",
    "    # Example parsing (adjust selectors for actual site)\n",
    "    # listing_data['Title'] = soup.select_one('.listing-title')?.text.strip()\n",
    "    # listing_data['price'] = soup.select_one('.price')?.text.strip()\n",
    "    # ... etc\n",
    "    \n",
    "    return listing_data\n",
    "\n",
    "\n",
    "def scrape_listings(urls: list, delay: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape multiple listing URLs.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of URLs to scrape\n",
    "        delay: Delay between requests in seconds\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with scraped listings\n",
    "    \"\"\"\n",
    "    listings = []\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        logger.info(f\"Scraping {i}/{len(urls)}: {url}\")\n",
    "        \n",
    "        html = fetch_page(url)\n",
    "        if html:\n",
    "            listing_data = parse_listing(html, url)\n",
    "            listings.append(listing_data)\n",
    "        \n",
    "        # Respectful delay\n",
    "        if i < len(urls):\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    df = pd.DataFrame(listings)\n",
    "    logger.info(f\"Successfully scraped {len(df)} listings\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ab911",
   "metadata": {},
   "source": [
    "## Step 3: Execute Scraping\n",
    "\n",
    "**Note:** This is a template. Update the `parse_listing()` function with actual CSS selectors for your target website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute scraping (uncomment when ready)\n",
    "# scraped_df = scrape_listings(listing_urls[:10], delay=DELAY)  # Start with first 10\n",
    "# display(scraped_df.head())\n",
    "\n",
    "# For demonstration, create sample data structure\n",
    "sample_data = {\n",
    "    'URL': ['https://example.com/listing1', 'https://example.com/listing2'],\n",
    "    'Title': ['Porsche 911 Carrera', 'Porsche Boxster S'],\n",
    "    'Model': ['911', 'Boxster'],\n",
    "    'Series': ['991', '987'],\n",
    "    'Year of construction': [2015, 2008],\n",
    "    'Mileage': ['45,000 km', '72,000 km'],\n",
    "    'price': [85000, 35000],\n",
    "    'currency': ['EUR', 'EUR'],\n",
    "    'Transmission': ['Manual', 'Manual'],\n",
    "    'Scraped_At': [datetime.now().isoformat()] * 2\n",
    "}\n",
    "\n",
    "scraped_df = pd.DataFrame(sample_data)\n",
    "print(f\"Scraped {len(scraped_df)} listings\")\n",
    "scraped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e69213",
   "metadata": {},
   "source": [
    "## Step 4: Save to Bronze Layer\n",
    "\n",
    "Save raw scraped data to the Bronze layer (raw data zone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ca7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path\n",
    "bronze_file = DATA_DIR_BRONZE / f\"listings_bronze_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "\n",
    "# Save using utility function\n",
    "save_data(scraped_df, bronze_file)\n",
    "\n",
    "print(f\"âœ“ Saved {len(scraped_df)} listings to Bronze layer\")\n",
    "print(f\"  Location: {bronze_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f5499",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Listings scraped:** {len(scraped_df)}\n",
    "- **Bronze file:** {bronze_file.name}\n",
    "\n",
    "**Next Step:** Run `02_bronze_to_silver.ipynb` to clean and standardize this data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
