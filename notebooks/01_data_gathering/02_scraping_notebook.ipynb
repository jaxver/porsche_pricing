{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Locate project root so we can import config.py regardless of notebook launch directory\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "while PROJECT_ROOT != PROJECT_ROOT.parent and not (PROJECT_ROOT / \"config.py\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "if not (PROJECT_ROOT / \"config.py\").exists():\n",
    "    raise FileNotFoundError(\"Unable to locate config.py from this notebook. Please run from within the repo.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config import (\n",
    "    ALL_LISTING_URLS_FILE,\n",
    "    LISTINGS_BRONZE,\n",
    "    LOGS_DIR,\n",
    "    SCRAPING_CONFIG,\n",
    ")\n",
    "from elferspot_listings.utils.helpers import ensure_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized paths for URL registry, bronze output, and logging\n",
    "ALL_LISTING_URLS_FILE = Path(ALL_LISTING_URLS_FILE)\n",
    "LISTINGS_BRONZE = Path(LISTINGS_BRONZE)\n",
    "LOGS_DIR = Path(LOGS_DIR)\n",
    "CHANGELOG_DIR = PROJECT_ROOT / \"changelogs\"\n",
    "\n",
    "ensure_dir(ALL_LISTING_URLS_FILE.parent)\n",
    "ensure_dir(LISTINGS_BRONZE.parent)\n",
    "ensure_dir(LOGS_DIR)\n",
    "ensure_dir(CHANGELOG_DIR)\n",
    "\n",
    "DEBUG_OUTPUT_PATH = LOGS_DIR / \"scraper_debug_output.html\"\n",
    "\n",
    "print(f\"URL registry ‚Üí {ALL_LISTING_URLS_FILE}\")\n",
    "print(f\"Bronze output ‚Üí {LISTINGS_BRONZE}\")\n",
    "print(f\"Changelogs ‚Üí {CHANGELOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the WebDriver (Selenium)\n",
    "driver_path = os.getenv(\"CHROMEDRIVER_PATH\", r'Z:\\Python\\chromedriver-win64\\chromedriver.exe')\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "url = \"https://www.elferspot.com/en/search/?series%5B%5D=911-f-model&series%5B%5D=912&series%5B%5D=911-g-model&series%5B%5D=930&series%5B%5D=964&series%5B%5D=993&series%5B%5D=996&series%5B%5D=997&series%5B%5D=991&series%5B%5D=992&series%5B%5D=911-backdate-modified&series%5B%5D=981&series%5B%5D=982&series%5B%5D=718\"\n",
    "driver.get(url)\n",
    "time.sleep(1.5)\n",
    "\n",
    "# Step 1: Handle the cookie popup\n",
    "try:\n",
    "    cookie_accept_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.brlbs-btn-accept-all'))\n",
    "    )\n",
    "    cookie_accept_button.click()\n",
    "    print(\"Cookie popup accepted.\")\n",
    "except:\n",
    "    print(\"No cookie popup found or it was not visible in time.\")\n",
    "\n",
    "# Step 2: Scroll the page one full screen height\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Step 3: Handle the second popup (if it appears)\n",
    "try:\n",
    "    dont_show_again_link = WebDriverWait(driver, 8).until(\n",
    "        EC.element_to_be_clickable((By.ID, 'popover_close'))\n",
    "    )\n",
    "    dont_show_again_link.click()\n",
    "    print(\"Search agent popup closed.\")\n",
    "except:\n",
    "    print(\"No 'Save this search' popup found or it was not visible in time.\")\n",
    "\n",
    "# Step 4: Continue scrolling and waiting for results to load (full-page scrolls)\n",
    "for _ in tqdm(range(100), desc=\"Scrolling and loading more listings\"):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Step 5: Extract all the listing URLs\n",
    "listing_urls = []\n",
    "try:\n",
    "    car_listings = driver.find_elements(By.CSS_SELECTOR, 'a.content-teaser')\n",
    "    for listing in car_listings:\n",
    "        url = listing.get_attribute(\"href\")\n",
    "        if url:\n",
    "            listing_urls.append(url)\n",
    "\n",
    "    print(f\"Found {len(listing_urls)} listing URLs:\")\n",
    "\n",
    "    prev_path = ALL_LISTING_URLS_FILE\n",
    "    if prev_path.exists():\n",
    "        prev_urls = set(pd.read_csv(prev_path)[\"Listing_URL\"].tolist())\n",
    "        new_urls = set(listing_urls) - prev_urls\n",
    "        print(f\"New URLs not in previous file: {len(new_urls)}\")\n",
    "    else:\n",
    "        print(\"No previous URL file found. All URLs are new.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting listing URLs: {str(e)}\")\n",
    "\n",
    "# Step 6: Create a pandas DataFrame and save to CSV\n",
    "df = pd.DataFrame(listing_urls, columns=['Listing_URL'])\n",
    "df.to_csv(ALL_LISTING_URLS_FILE, index=False)\n",
    "print(f\"Listing URLs have been saved to '{ALL_LISTING_URLS_FILE}'.\")\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "REQUEST_TIMEOUT = SCRAPING_CONFIG.get('request_timeout', 10)\n",
    "MAX_RETRIES = SCRAPING_CONFIG.get('max_retries', 3)\n",
    "REQUEST_DELAY = SCRAPING_CONFIG.get('delay_between_requests', 1.0)\n",
    "DEFAULT_MAX_WORKERS = min(16, (os.cpu_count() or 8))\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": SCRAPING_CONFIG.get(\n",
    "        'user_agent',\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    \"borlabs-cookie\": \"%7B%22consents%22%3A%7B%22essential%22%3A%5B%22borlabs-cookie%22%2C%22vg-wort%22%2C%22woocommerce%22%2C%22wordfence%22%2C%22wpml%22%5D%2C%22statistics%22%3A%5B%22google-analytics-four%22%2C%22woocommerce-google-analytics%22%5D%2C%22marketing%22%3A%5B%22google-ads%22%2C%22meta-pixel-for-woocommerce%22%2C%22taboola%22%2C%22wc-order-attribution%22%5D%2C%22external-media%22%3A%5B%22pinterest%22%2C%22vimeo%22%2C%22youtube%22%5D%7D%2C%22domainPath%22%3A%22www.elferspot.com%2Fen%2F%22%2C%22expires%22%3A%22Wed%2C%2005%20Nov%202025%2016%3A12%3A58%20GMT%22%2C%22uid%22%3A%22m8drsv73-w32f3qy0-h1xdf0ij-1393u9z0%22%2C%22v3%22%3Atrue%2C%22version%22%3A2%7D\",\n",
    "    \"atom-no-inquiry-popover\": \"true\",\n",
    "    \"wp-wpml_current_admin_language_d41d8cd98f00b204e9800998ecf8427e\": \"en\",\n",
    "    \"wp-wpml_current_language\": \"en\"\n",
    "}\n",
    "\n",
    "FIELDS_TO_HASH = [\"Title\", \"Price\", \"Mileage\", \"Description\", \"Secondary_Description\"]\n",
    "\n",
    "def compute_hash(row, fields):\n",
    "    row_data = {field: row.get(field, \"\") for field in fields}\n",
    "    row_json = json.dumps(row_data, sort_keys=True)\n",
    "    return hashlib.md5(row_json.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_field_changes(old_row, new_row, fields):\n",
    "    changes = []\n",
    "    for field in fields:\n",
    "        old = old_row.get(field, \"\")\n",
    "        new = new_row.get(field, \"\")\n",
    "        if old != new:\n",
    "            changes.append(f\"{field}: '{old}' ‚Üí '{new}'\")\n",
    "    return \"; \".join(changes) if changes else None\n",
    "\n",
    "def parse_listing(url):\n",
    "    last_error = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = session.get(\n",
    "                url, headers=HEADERS, cookies=cookies, timeout=REQUEST_TIMEOUT\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                break\n",
    "            last_error = f\"Status {r.status_code}\"\n",
    "        except requests.RequestException as exc:\n",
    "            last_error = str(exc)\n",
    "            r = None\n",
    "        time.sleep(REQUEST_DELAY + random.uniform(0.1, 0.5))\n",
    "    else:\n",
    "        return {\"URL\": url, \"Error\": f\"Failed after {MAX_RETRIES} attempts: {last_error}\", \"Scraped_At\": datetime.today().date()}\n",
    "\n",
    "    DEBUG_OUTPUT_PATH.write_text(r.text, encoding=\"utf-8\")\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    data = {\"URL\": url, \"Scraped_At\": datetime.today().date()}\n",
    "\n",
    "    title_tag = soup.find(\"h1\")\n",
    "    if title_tag:\n",
    "        data[\"Title\"] = title_tag.get_text(strip=True)\n",
    "\n",
    "    main_desc = None\n",
    "    overview_section = soup.find(\"section\", id=\"overview_anchor\")\n",
    "    if overview_section:\n",
    "        main_col = overview_section.find(\"div\", class_=\"col-xs-12 col-md-8\")\n",
    "        if main_col:\n",
    "            content_div = main_col.find(\"div\", class_=\"content\")\n",
    "            if content_div:\n",
    "                main_desc_paragraphs = [\n",
    "                    p.get_text(strip=True)\n",
    "                    for p in content_div.find_all(\"p\")\n",
    "                    if \"small\" not in (p.get(\"class\") or [])\n",
    "                ]\n",
    "                if main_desc_paragraphs:\n",
    "                    main_desc = \"\\n\".join(main_desc_paragraphs)\n",
    "    if main_desc:\n",
    "        data[\"Description\"] = main_desc\n",
    "\n",
    "    secondary_desc = None\n",
    "    maincontent_section = soup.find(\"section\", class_=\"maincontent\")\n",
    "    if maincontent_section:\n",
    "        for row in maincontent_section.find_all(\"div\", class_=\"row\"):\n",
    "            for col in row.find_all(\"div\", class_=\"col-xs-12\"):\n",
    "                if \"col-md-8\" in (col.get(\"class\") or []):\n",
    "                    continue\n",
    "                content_div = col.find(\"div\", class_=\"content\")\n",
    "                if content_div:\n",
    "                    sec_desc_parts = []\n",
    "                    for elem in content_div.find_all([\"p\", \"ul\"], recursive=False):\n",
    "                        if elem.name == \"p\":\n",
    "                            if \"small\" not in (elem.get(\"class\") or []):\n",
    "                                sec_desc_parts.append(elem.get_text(strip=True))\n",
    "                        elif elem.name == \"ul\":\n",
    "                            items = [li.get_text(strip=True) for li in elem.find_all(\"li\")]\n",
    "                            if items:\n",
    "                                sec_desc_parts.append(\"\\n\".join(items))\n",
    "                    if sec_desc_parts:\n",
    "                        secondary_desc = \"\\n\".join(sec_desc_parts)\n",
    "                        break\n",
    "            if secondary_desc:\n",
    "                break\n",
    "    if secondary_desc:\n",
    "        data[\"Secondary_Description\"] = secondary_desc\n",
    "\n",
    "    spec_tables = soup.select(\"table.fahrzeugdaten\")\n",
    "    for table in spec_tables:\n",
    "        for row in table.select(\"tr\"):\n",
    "            label = row.select_one(\"td.label\")\n",
    "            content = row.select_one(\"td.content\")\n",
    "            if label and content:\n",
    "                key = label.get_text(strip=True).rstrip(\":\")\n",
    "                value = content.get_text(strip=True)\n",
    "                data[key] = value\n",
    "\n",
    "    price_tag = soup.select_one(\"div.sidebar-section .price span.p\")\n",
    "    if price_tag:\n",
    "        data[\"Price\"] = price_tag.get_text(strip=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_all_listings(listing_urls, max_workers=DEFAULT_MAX_WORKERS):\n",
    "    listings_data = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(parse_listing, url): url for url in listing_urls}\n",
    "        for future in tqdm(as_completed(future_to_url), total=len(listing_urls), desc=\"Parsing listings\"):\n",
    "            listing_data = future.result()\n",
    "            listings_data.append(listing_data)\n",
    "    return pd.DataFrame(listings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = LISTINGS_BRONZE\n",
    "if not ALL_LISTING_URLS_FILE.exists():\n",
    "    raise FileNotFoundError(f\"URL registry missing at {ALL_LISTING_URLS_FILE}. Run the scrolling cell first.\")\n",
    "\n",
    "df_urls = pd.read_csv(ALL_LISTING_URLS_FILE)\n",
    "all_urls = set(df_urls['Listing_URL'].dropna())\n",
    "\n",
    "if bronze_path.exists():\n",
    "    df_existing = pd.read_excel(bronze_path)\n",
    "else:\n",
    "    df_existing = pd.DataFrame(columns=[\"URL\"])\n",
    "\n",
    "if 'URL' not in df_existing.columns:\n",
    "    df_existing['URL'] = df_existing.get('URL', pd.Series(dtype=str))\n",
    "\n",
    "df_existing.set_index(\"URL\", inplace=True, drop=False)\n",
    "existing_urls = set(df_existing.index)\n",
    "new_urls = sorted(all_urls - existing_urls)\n",
    "\n",
    "if new_urls:\n",
    "    print(f\"üîç Found {len(new_urls)} new listings (not in {bronze_path.name}). Parsing...\")\n",
    "    df_new = parse_all_listings(new_urls, max_workers=DEFAULT_MAX_WORKERS)\n",
    "    df_new['Data_Hash'] = df_new.apply(lambda row: compute_hash(row, FIELDS_TO_HASH), axis=1)\n",
    "    df_new.set_index(\"URL\", inplace=True)\n",
    "    df_existing = pd.concat([df_existing, df_new], axis=0)\n",
    "    print(f\"‚úÖ Appended {len(df_new)} new listings.\")\n",
    "else:\n",
    "    print(\"No new listings detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_urls = sorted(existing_urls & all_urls)\n",
    "change_log = []\n",
    "if update_urls:\n",
    "    print(f\"üîÑ Checking {len(update_urls)} existing listings for updates...\")\n",
    "    df_updates = parse_all_listings(update_urls, max_workers=DEFAULT_MAX_WORKERS)\n",
    "    df_updates.set_index(\"URL\", inplace=True)\n",
    "    for url in update_urls:\n",
    "        if url not in df_existing.index or url not in df_updates.index:\n",
    "            continue\n",
    "        old_row = df_existing.loc[url].to_dict()\n",
    "        new_row = df_updates.loc[url].to_dict()\n",
    "        if new_row.get(\"Error\"):\n",
    "            print(f\"‚ö†Ô∏è Skipping update for {url} due to error: {new_row['Error']}\")\n",
    "            continue\n",
    "        old_hash = old_row.get(\"Data_Hash\", \"\")\n",
    "        new_hash = compute_hash(new_row, FIELDS_TO_HASH)\n",
    "        if old_hash == new_hash:\n",
    "            continue\n",
    "        changes = get_field_changes(old_row, new_row, FIELDS_TO_HASH)\n",
    "        price_changed = old_row.get(\"Price\", \"\") != new_row.get(\"Price\", \"\")\n",
    "        only_price_changed = price_changed and all(\n",
    "            old_row.get(f, \"\") == new_row.get(f, \"\") for f in FIELDS_TO_HASH if f != \"Price\"\n",
    "        )\n",
    "        new_price = str(new_row.get(\"Price\", \"\")).strip().lower()\n",
    "        if only_price_changed and new_price in {\"reserved\", \"\", \"n/a\"}:\n",
    "            continue\n",
    "        for col in df_existing.columns:\n",
    "            if col in new_row:\n",
    "                try:\n",
    "                    dtype = df_existing[col].dtype\n",
    "                    value = new_row[col]\n",
    "                    if pd.isna(value):\n",
    "                        continue\n",
    "                    if \"datetime\" in str(dtype):\n",
    "                        new_row[col] = pd.to_datetime(value, errors=\"coerce\")\n",
    "                    elif \"float\" in str(dtype):\n",
    "                        new_row[col] = float(value)\n",
    "                    elif \"int\" in str(dtype):\n",
    "                        new_row[col] = int(float(value))\n",
    "                    else:\n",
    "                        new_row[col] = str(value)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        new_row['Data_Hash'] = new_hash\n",
    "        df_existing.loc[url] = pd.Series(new_row)\n",
    "        change_log.append({\n",
    "            \"URL\": url,\n",
    "            \"Changes\": changes or \"hash-only update\",\n",
    "            \"Date\": datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        })\n",
    "    if change_log:\n",
    "        log_file = CHANGELOG_DIR / f\"changelog_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.xlsx\"\n",
    "        pd.DataFrame(change_log).to_excel(log_file, index=False)\n",
    "        print(f\"üìù Saved changelog with {len(change_log)} updates to {log_file}\")\n",
    "else:\n",
    "    print(\"No existing listings required updates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing = df_existing[~df_existing.index.duplicated(keep='last')]\n",
    "df_existing = df_existing.sort_index()\n",
    "df_existing.reset_index(drop=True, inplace=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "snapshot_path = LISTINGS_BRONZE.parent / f\"{LISTINGS_BRONZE.stem}_{timestamp}.xlsx\"\n",
    "\n",
    "df_existing.to_excel(LISTINGS_BRONZE, index=False)\n",
    "df_existing.to_excel(snapshot_path, index=False)\n",
    "print(\n",
    "    f\"‚úÖ Updated listing data saved to '{LISTINGS_BRONZE}' and snapshot '{snapshot_path.name}'. Total listings: {len(df_existing)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_descriptions_parallel(df, max_workers=DEFAULT_MAX_WORKERS):\n",
    "    urls = df[\"URL\"].dropna().tolist()\n",
    "    if not urls:\n",
    "        return df\n",
    "    def fetch_desc(url):\n",
    "        try:\n",
    "            parsed = parse_listing(url)\n",
    "            return url, parsed.get(\"Description\"), parsed.get(\"Secondary_Description\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to update {url}: {exc}\")\n",
    "            return url, None, None\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_desc, url): url for url in urls}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Updating descriptions\"):\n",
    "            url, desc, sec_desc = future.result()\n",
    "            mask = df[\"URL\"] == url\n",
    "            if desc is not None:\n",
    "                df.loc[mask, \"Description\"] = desc\n",
    "            if sec_desc is not None:\n",
    "                df.loc[mask, \"Secondary_Description\"] = sec_desc\n",
    "    return df\n",
    "\n",
    "if 'df_existing' not in globals():\n",
    "    if LISTINGS_BRONZE.exists():\n",
    "        df_existing = pd.read_excel(LISTINGS_BRONZE)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Bronze dataset not found. Run scraping cells first.\")\n",
    "\n",
    "df_existing = update_descriptions_parallel(df_existing)\n",
    "refresh_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "refresh_snapshot = LISTINGS_BRONZE.parent / f\"{LISTINGS_BRONZE.stem}_descriptions_{refresh_timestamp}.xlsx\"\n",
    "df_existing.to_excel(LISTINGS_BRONZE, index=False)\n",
    "df_existing.to_excel(refresh_snapshot, index=False)\n",
    "print(\n",
    "    f\"‚úÖ Descriptions refreshed at '{LISTINGS_BRONZE}' and snapshot '{refresh_snapshot.name}'.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
