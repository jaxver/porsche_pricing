{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "project_root = Path.cwd().parents[1]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config import RESULTS_DIR, LISTINGS_GOLD\n",
    "from elferspot_listings.utils.helpers import load_data\n",
    "\n",
    "PREDICTIONS_DIR = RESULTS_DIR / 'model_predictions'\n",
    "MODELS = ['catboost', 'ridge', 'elasticnet']\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "def latest_prediction_path(model_name: str) -> Path:\n",
    "    pattern = f\"{model_name}_predictions_*.xlsx\"\n",
    "    files = sorted(PREDICTIONS_DIR.glob(pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No prediction exports found for {model_name}. Run notebooks in 04_modeling/ first.\"\n",
    "        )\n",
    "    return files[-1]\n",
    "\n",
    "def load_predictions(model_name: str) -> pd.DataFrame:\n",
    "    file_path = latest_prediction_path(model_name)\n",
    "    df = pd.read_excel(file_path, sheet_name='all_results')\n",
    "    df['model'] = model_name\n",
    "    df['prediction_file'] = file_path.name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7b280",
   "metadata": {},
   "source": [
    "## Load prediction exports\n",
    "Bring in the latest `model_predictions/*.xlsx` artifacts for CatBoost, Ridge, and ElasticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs: Dict[str, pd.DataFrame] = {}\n",
    "for model_name in MODELS:\n",
    "    try:\n",
    "        model_dfs[model_name] = load_predictions(model_name)\n",
    "        print(f\"Loaded {len(model_dfs[model_name]):,} rows for {model_name} from {model_dfs[model_name]['prediction_file'].iloc[0]}\")\n",
    "    except FileNotFoundError as exc:\n",
    "        print(exc)\n",
    "        raise\n",
    "\n",
    "combined_df = pd.concat(model_dfs.values(), ignore_index=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07157d18",
   "metadata": {},
   "source": [
    "## Compare model performance\n",
    "Aggregate MAE/RMSE/R² per model on the shared hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rows = []\n",
    "for model_name, df in model_dfs.items():\n",
    "    residuals = df['price_in_eur'] - df['pred_price']\n",
    "    mae = residuals.abs().mean()\n",
    "    rmse = np.sqrt(np.mean(np.square(residuals)))\n",
    "    ss_res = np.sum(np.square(residuals))\n",
    "    ss_tot = np.sum(np.square(df['price_in_eur'] - df['price_in_eur'].mean()))\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    coverage = (\n",
    "        ((df['price_in_eur'] >= df['pred_lower']) & (df['price_in_eur'] <= df['pred_upper']))\n",
    "        .mean()\n",
    "    )\n",
    "    metrics_rows.append({\n",
    "        'model': model_name,\n",
    "        'mae_eur': mae,\n",
    "        'rmse_eur': rmse,\n",
    "        'r2': r2,\n",
    "        'interval_coverage': coverage,\n",
    "        'sample_size': len(df),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values('rmse_eur')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2db15f",
   "metadata": {},
   "source": [
    "## Residual diagnostics\n",
    "Visualise actual vs predicted prices and residual spreads per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['residual_price'] = combined_df['price_in_eur'] - combined_df['pred_price']\n",
    "\n",
    "fig_scatter = px.scatter(\n",
    "    combined_df,\n",
    "    x='price_in_eur',\n",
    "    y='pred_price',\n",
    "    color='model',\n",
    "    hover_name='Title' if 'Title' in combined_df.columns else None,\n",
    "    hover_data=['Model', 'Series', 'Car location'],\n",
    "    title='Actual vs Predicted Price by Model',\n",
    "    opacity=0.65,\n",
    "    trendline='ols',\n",
    "    labels={'price_in_eur': 'Actual Price (EUR)', 'pred_price': 'Predicted Price (EUR)'}\n",
    ")\n",
    "fig_scatter.add_shape(type='line', x0=combined_df['price_in_eur'].min(), x1=combined_df['price_in_eur'].max(), y0=combined_df['price_in_eur'].min(), y1=combined_df['price_in_eur'].max(), line=dict(color='black', dash='dash'))\n",
    "fig_scatter.show()\n",
    "\n",
    "fig_box = px.box(\n",
    "    combined_df,\n",
    "    x='model',\n",
    "    y='residual_price',\n",
    "    title='Residual distribution (EUR)',\n",
    "    points='suspectedoutliers',\n",
    "    labels={'residual_price': 'Residual (Actual - Pred, EUR)'},\n",
    "    color='model'\n",
    ")\n",
    "fig_box.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50195c96",
   "metadata": {},
   "source": [
    "## Under/over-valued watchlist\n",
    "Flag listings whose actual price breaches the model prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "underpriced = combined_df[combined_df['price_in_eur'] < combined_df['pred_lower']].copy()\n",
    "overpriced = combined_df[combined_df['price_in_eur'] > combined_df['pred_upper']].copy()\n",
    "\n",
    "def summarize_watchlist(df: pd.DataFrame, label: str, n: int = 10) -> pd.DataFrame:\n",
    "    cols = ['model', 'price_in_eur', 'pred_price', 'pred_lower', 'pred_upper', 'residual_price', 'Title', 'Model', 'Series', 'Car location']\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    summary = df.sort_values('residual_price').head(n) if label == 'Underpriced' else df.sort_values('residual_price', ascending=False).head(n)\n",
    "    summary = summary[cols].copy()\n",
    "    summary['price_delta_pct'] = summary['residual_price'] / summary['pred_price']\n",
    "    summary['label'] = label\n",
    "    return summary\n",
    "\n",
    "underpriced_summary = summarize_watchlist(underpriced, 'Underpriced')\n",
    "overpriced_summary = summarize_watchlist(overpriced, 'Overpriced')\n",
    "display(underpriced_summary)\n",
    "display(overpriced_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dd22e",
   "metadata": {},
   "source": [
    "## Notes & next steps\n",
    "- CatBoost typically tops RMSE, but Ridge/ElasticNet provide interpretable baselines—promote whichever satisfies coverage targets.\n",
    "- Exported watchlists can be pushed to CRM or dashboard (see `app/streamlit_app.py`).\n",
    "- Re-run notebooks in `04_modeling/` before this analysis whenever Gold data refreshes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
