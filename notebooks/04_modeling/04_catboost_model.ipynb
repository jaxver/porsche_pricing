{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c096245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config import (DATA_DIR_GOLD, MODELS_DIR, MODEL_CATBOOST, \n",
    "                    MODEL_CONFIG, CATEGORICAL_FEATURES, TARGET_VARIABLE)\n",
    "from elferspot_listings.utils.helpers import setup_logging, load_data\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(level='INFO')\n",
    "logger.info(\"CatBoost modeling initialized\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = MODEL_CONFIG['random_state']\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4112732",
   "metadata": {},
   "source": [
    "## Step 1: Load Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5724aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent gold file\n",
    "gold_files = sorted(DATA_DIR_GOLD.glob(\"listings_gold*.xlsx\"))\n",
    "\n",
    "if gold_files:\n",
    "    gold_path = gold_files[-1]\n",
    "    logger.info(f\"Using gold file: {gold_path.name}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No gold files found in {DATA_DIR_GOLD}\")\n",
    "\n",
    "# Load data\n",
    "df_gold = load_data(gold_path)\n",
    "print(f\"✓ Loaded {len(df_gold):,} rows from gold layer\")\n",
    "print(f\"  File: {gold_path.name}\")\n",
    "\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d866d",
   "metadata": {},
   "source": [
    "## Step 2: Define Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "numeric_features = [\n",
    "    'Mileage_km',\n",
    "    'Mileage_sq',\n",
    "    'log_mileage',\n",
    "    'listing_score',\n",
    "    'Year of construction'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Series',\n",
    "    'model_category',\n",
    "    'Transmission',\n",
    "    'Drive',\n",
    "    'Ready to drive',\n",
    "    'Car location',\n",
    "    'Matching numbers',\n",
    "    'Interior color',\n",
    "    'Paint-to-Sample (PTS)',\n",
    "    'is_fully_restored',\n",
    "    'owners_known'\n",
    "]\n",
    "\n",
    "# Filter to only available columns\n",
    "numeric_features = [f for f in numeric_features if f in df_gold.columns]\n",
    "categorical_features = [f for f in categorical_features if f in df_gold.columns]\n",
    "\n",
    "all_features = numeric_features + categorical_features\n",
    "target = 'log_price' if 'log_price' in df_gold.columns else 'price_in_eur'\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"\\nTarget variable: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd48834",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Modeling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594afe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing target\n",
    "df_model = df_gold[all_features + [target]].copy()\n",
    "df_model = df_model.dropna(subset=[target])\n",
    "\n",
    "print(f\"Dataset shape: {df_model.shape}\")\n",
    "print(f\"Missing values:\\n{df_model.isnull().sum()[df_model.isnull().sum() > 0]}\")\n",
    "\n",
    "# Fill missing values in features\n",
    "for col in categorical_features:\n",
    "    if col in df_model.columns:\n",
    "        df_model[col] = df_model[col].fillna('Unknown').astype(str)\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col in df_model.columns:\n",
    "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "\n",
    "print(f\"\\n✓ Prepared {len(df_model):,} samples for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7620c9",
   "metadata": {},
   "source": [
    "## Step 4: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d74765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df_model[all_features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=MODEL_CONFIG['test_size'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"Split ratio: {MODEL_CONFIG['test_size']*100:.0f}% test\")\n",
    "\n",
    "# Identify categorical feature indices\n",
    "cat_feature_indices = [X_train.columns.get_loc(col) for col in categorical_features if col in X_train.columns]\n",
    "print(f\"\\nCategorical feature indices: {len(cat_feature_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc51ad",
   "metadata": {},
   "source": [
    "## Step 5: Train CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e09599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CatBoost model with config\n",
    "model = CatBoostRegressor(\n",
    "    iterations=MODEL_CONFIG['catboost']['iterations'],\n",
    "    learning_rate=MODEL_CONFIG['catboost']['learning_rate'],\n",
    "    depth=MODEL_CONFIG['catboost']['depth'],\n",
    "    l2_leaf_reg=MODEL_CONFIG['catboost']['l2_leaf_reg'],\n",
    "    random_seed=MODEL_CONFIG['catboost']['random_seed'],\n",
    "    cat_features=cat_feature_indices,\n",
    "    verbose=100,\n",
    "    eval_metric='RMSE',\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "print(\"Training CatBoost model...\")\n",
    "print(f\"Configuration: {MODEL_CONFIG['catboost']}\")\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92478a",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9110df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, label=''):\n",
    "    \"\"\"Calculate and display regression metrics.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{label} Metrics:\")\n",
    "    print(f\"  RMSE: {rmse:,.2f}\")\n",
    "    print(f\"  MAE:  {mae:,.2f}\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, 'Training')\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, 'Test')\n",
    "\n",
    "# Check for overfitting\n",
    "print(f\"\\nOverfitting check:\")\n",
    "print(f\"  R² difference: {train_metrics['R2'] - test_metrics['R2']:.4f}\")\n",
    "if train_metrics['R2'] - test_metrics['R2'] > 0.1:\n",
    "    print(\"  ⚠️ Possible overfitting detected\")\n",
    "else:\n",
    "    print(\"  ✓ Model generalization looks good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092beed",
   "metadata": {},
   "source": [
    "## Step 7: Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Predicted vs Actual (Test set)\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'Predicted vs Actual (Test)\\nR² = {test_metrics[\"R2\"]:.4f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.4f}\")\n",
    "print(f\"  Std:  {residuals.std():.4f}\")\n",
    "print(f\"  Min:  {residuals.min():.4f}\")\n",
    "print(f\"  Max:  {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135a32f",
   "metadata": {},
   "source": [
    "## Step 8: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47235e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== Top 20 Most Important Features ===\")\n",
    "display(feature_importance.head(20))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1933684",
   "metadata": {},
   "source": [
    "## Step 9: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "\n",
    "# Create Pool for CatBoost CV\n",
    "cv_data = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    cat_features=cat_feature_indices\n",
    ")\n",
    "\n",
    "# CV parameters\n",
    "cv_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': MODEL_CONFIG['catboost']['learning_rate'],\n",
    "    'depth': MODEL_CONFIG['catboost']['depth'],\n",
    "    'l2_leaf_reg': MODEL_CONFIG['catboost']['l2_leaf_reg'],\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Run CV\n",
    "cv_results = cv(\n",
    "    cv_data,\n",
    "    cv_params,\n",
    "    fold_count=5,\n",
    "    shuffle=True,\n",
    "    partition_random_seed=RANDOM_SEED,\n",
    "    plot=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Display CV results\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"  Mean RMSE: {cv_results['test-RMSE-mean'].iloc[-1]:.4f}\")\n",
    "print(f\"  Std RMSE:  {cv_results['test-RMSE-std'].iloc[-1]:.4f}\")\n",
    "\n",
    "# Plot CV learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cv_results['test-RMSE-mean'], label='CV Mean RMSE')\n",
    "plt.fill_between(\n",
    "    range(len(cv_results)),\n",
    "    cv_results['test-RMSE-mean'] - cv_results['test-RMSE-std'],\n",
    "    cv_results['test-RMSE-mean'] + cv_results['test-RMSE-std'],\n",
    "    alpha=0.3\n",
    ")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Cross-Validation Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309c159",
   "metadata": {},
   "source": [
    "## Step 10: Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7200f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_path = MODELS_DIR / f\"catboost_model_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.cbm\"\n",
    "model.save_model(str(model_path))\n",
    "\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = MODELS_DIR / f\"feature_importance_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "feature_importance.to_csv(importance_path, index=False)\n",
    "print(f\"✓ Feature importance saved to: {importance_path}\")\n",
    "\n",
    "# Save model metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'metric': ['RMSE', 'MAE', 'R2'],\n",
    "    'train': [train_metrics['RMSE'], train_metrics['MAE'], train_metrics['R2']],\n",
    "    'test': [test_metrics['RMSE'], test_metrics['MAE'], test_metrics['R2']]\n",
    "})\n",
    "metrics_path = MODELS_DIR / f\"model_metrics_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"✓ Model metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e08a6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ **CatBoost Model Training Complete**\n",
    "\n",
    "### Model Performance\n",
    "- **Test R²:** {test_metrics['R2']:.4f}\n",
    "- **Test RMSE:** {test_metrics['RMSE']:,.2f}\n",
    "- **Test MAE:** {test_metrics['MAE']:,.2f}\n",
    "\n",
    "### Files Saved\n",
    "- Model: `{model_path.name}`\n",
    "- Feature Importance: `{importance_path.name}`\n",
    "- Metrics: `{metrics_path.name}`\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare with other models (Ridge, ElasticNet) in `04_model_comparison.ipynb`\n",
    "- Use model for predictions in analysis notebooks\n",
    "- Deploy model in Streamlit app"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
