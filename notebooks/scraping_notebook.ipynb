{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import optuna\n",
    "\n",
    "#Heatmap and VIF\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_log_dir = \"../changelogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie popup accepted.\n",
      "Search agent popup closed.\n",
      "Search agent popup closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrolling and loading more listings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:35<00:00,  1.55s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3520 listing URLs:\n",
      "New URLs not in previous file: 911\n",
      "Listing URLs have been saved to 'all_listing_urls.csv'.\n",
      "New URLs not in previous file: 911\n",
      "Listing URLs have been saved to 'all_listing_urls.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver (Selenium)\n",
    "# Path to your WebDriver (e.g., chromedriver)\n",
    "driver_path = r'Z:\\Python\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "# Create a Service object with the path to your chromedriver\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Initialize the WebDriver with the Service object\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the webpage\n",
    "url = \"https://www.elferspot.com/en/search/?series%5B%5D=911-f-model&series%5B%5D=912&series%5B%5D=911-g-model&series%5B%5D=930&series%5B%5D=964&series%5B%5D=993&series%5B%5D=996&series%5B%5D=997&series%5B%5D=991&series%5B%5D=992&series%5B%5D=911-backdate-modified&series%5B%5D=981&series%5B%5D=982&series%5B%5D=718\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load (you may need to adjust this for your page's behavior)\n",
    "time.sleep(1.5)\n",
    "\n",
    "# Step 1: Handle the cookie popup\n",
    "try:\n",
    "    cookie_accept_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.brlbs-btn-accept-all'))  # Adjust selector\n",
    "    )\n",
    "    cookie_accept_button.click()\n",
    "    print(\"Cookie popup accepted.\")\n",
    "except:\n",
    "    print(\"No cookie popup found or it was not visible in time.\")\n",
    "\n",
    "# Step 2: Scroll the page one full screen height\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(1)  # Wait for content to load\n",
    "\n",
    "# Step 3: Handle the second popup (if it appears)\n",
    "try:\n",
    "    dont_show_again_link = WebDriverWait(driver, 8).until(\n",
    "        EC.element_to_be_clickable((By.ID, 'popover_close'))  # Target the \"Don't show again\" link\n",
    "    )\n",
    "    dont_show_again_link.click()\n",
    "    print(\"Search agent popup closed.\")\n",
    "except:\n",
    "    print(\"No 'Save this search' popup found or it was not visible in time.\")\n",
    "\n",
    "# Step 4: Continue scrolling and waiting for results to load (full-page scrolls)\n",
    "for _ in tqdm(range(100), desc=\"Scrolling and loading more listings\"):\n",
    "    # Scroll by the height of the visible viewport (one full screen)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1.5)  # Wait for content to load\n",
    "\n",
    "# Step 5: Extract all the listing URLs\n",
    "listing_urls = []\n",
    "try:\n",
    "    # Find all elements that contain the listing links\n",
    "    car_listings = driver.find_elements(By.CSS_SELECTOR, 'a.content-teaser')  # Adjust the selector\n",
    "    for listing in car_listings:\n",
    "        url = listing.get_attribute(\"href\")\n",
    "        if url:\n",
    "            listing_urls.append(url)\n",
    "\n",
    "    # Print out the URLs\n",
    "    print(f\"Found {len(listing_urls)} listing URLs:\")\n",
    "\n",
    "    prev_path = \"../data/all_listing_urls.csv\"\n",
    "    if os.path.exists(prev_path):\n",
    "        prev_urls = set(pd.read_csv(prev_path)[\"Listing_URL\"].tolist())\n",
    "        new_urls = set(listing_urls) - prev_urls\n",
    "        print(f\"New URLs not in previous file: {len(new_urls)}\")\n",
    "    else:\n",
    "        print(\"No previous URL file found. All URLs are new.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting listing URLs: {str(e)}\")\n",
    "\n",
    "# Step 6: Create a pandas DataFrame and save to CSV\n",
    "df = pd.DataFrame(listing_urls, columns=['Listing_URL'])\n",
    "df.to_csv(\"../data/all_listing_urls.csv\", index=False)\n",
    "print(\"Listing URLs have been saved to 'all_listing_urls.csv'.\")\n",
    "\n",
    "# Wait to observe the results or continue interacting\n",
    "time.sleep(5)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session for persistent connection reuse\n",
    "session = requests.Session()\n",
    "\n",
    "# Headers for the requests\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    \"borlabs-cookie\": \"%7B%22consents%22%3A%7B%22essential%22%3A%5B%22borlabs-cookie%22%2C%22vg-wort%22%2C%22woocommerce%22%2C%22wordfence%22%2C%22wpml%22%5D%2C%22statistics%22%3A%5B%22google-analytics-four%22%2C%22woocommerce-google-analytics%22%5D%2C%22marketing%22%3A%5B%22google-ads%22%2C%22meta-pixel-for-woocommerce%22%2C%22taboola%22%2C%22wc-order-attribution%22%5D%2C%22external-media%22%3A%5B%22pinterest%22%2C%22vimeo%22%2C%22youtube%22%5D%7D%2C%22domainPath%22%3A%22www.elferspot.com%2Fen%2F%22%2C%22expires%22%3A%22Wed%2C%2005%20Nov%202025%2016%3A12%3A58%20GMT%22%2C%22uid%22%3A%22m8drsv73-w32f3qy0-h1xdf0ij-1393u9z0%22%2C%22v3%22%3Atrue%2C%22version%22%3A2%7D\",\n",
    "    \"atom-no-inquiry-popover\": \"true\",\n",
    "    \"wp-wpml_current_admin_language_d41d8cd98f00b204e9800998ecf8427e\": \"en\",\n",
    "    \"wp-wpml_current_language\": \"en\"\n",
    "}\n",
    "\n",
    "\n",
    "FIELDS_TO_HASH = [\"Title\", \"Price\",\"Mileage\",\"Description\",\"Secondary_Description\"] \n",
    "\n",
    "def compute_hash(row, fields):\n",
    "    row_data = {field: row.get(field, \"\") for field in fields}\n",
    "    row_json = json.dumps(row_data, sort_keys=True)\n",
    "    return hashlib.md5(row_json.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_field_changes(old_row, new_row, fields):\n",
    "    changes = []\n",
    "    for field in fields:\n",
    "        old = old_row.get(field, \"\")\n",
    "        new = new_row.get(field, \"\")\n",
    "        if old != new:\n",
    "            changes.append(f\"{field}: '{old}' ‚Üí '{new}'\")\n",
    "    return \"; \".join(changes) if changes else None\n",
    "\n",
    "def parse_listing(url):\n",
    "    r = session.get(url, headers=HEADERS, cookies=cookies)\n",
    "    with open(\"debug_output.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return {\"URL\": url, \"Error\": f\"Failed with status {r.status_code}\", \"Scraped_At\": datetime.today().date()}\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    data = {\"URL\": url, \"Scraped_At\": datetime.today().date()}\n",
    "\n",
    "    title_tag = soup.find(\"h1\")\n",
    "    if title_tag:\n",
    "        data[\"Title\"] = title_tag.get_text(strip=True)\n",
    "\n",
    "    # --- Parse main description from #overview_anchor .col-xs-12.col-md-8 > .content ---\n",
    "    main_desc = None\n",
    "    overview_section = soup.find(\"section\", id=\"overview_anchor\")\n",
    "    if overview_section:\n",
    "        main_col = overview_section.find(\"div\", class_=\"col-xs-12 col-md-8\")\n",
    "        if main_col:\n",
    "            content_div = main_col.find(\"div\", class_=\"content\")\n",
    "            if content_div:\n",
    "                main_desc_paragraphs = [\n",
    "                    p.get_text(strip=True)\n",
    "                    for p in content_div.find_all(\"p\")\n",
    "                    if \"small\" not in (p.get(\"class\") or [])\n",
    "                ]\n",
    "                if main_desc_paragraphs:\n",
    "                    main_desc = \"\\n\".join(main_desc_paragraphs)\n",
    "    if main_desc:\n",
    "        data[\"Description\"] = main_desc\n",
    "\n",
    "    # --- Parse secondary description from .maincontent .container .row .col-xs-12 > .content (not .col-md-8) ---\n",
    "    secondary_desc = None\n",
    "    maincontent_section = soup.find(\"section\", class_=\"maincontent\")\n",
    "    if maincontent_section:\n",
    "        for row in maincontent_section.find_all(\"div\", class_=\"row\"):\n",
    "            for col in row.find_all(\"div\", class_=\"col-xs-12\"):\n",
    "                # Exclude .col-md-8 columns (already parsed above)\n",
    "                if \"col-md-8\" in (col.get(\"class\") or []):\n",
    "                    continue\n",
    "                content_div = col.find(\"div\", class_=\"content\")\n",
    "                if content_div:\n",
    "                    sec_desc_parts = []\n",
    "                    for elem in content_div.find_all([\"p\", \"ul\"], recursive=False):\n",
    "                        if elem.name == \"p\":\n",
    "                            if \"small\" not in (elem.get(\"class\") or []):\n",
    "                                sec_desc_parts.append(elem.get_text(strip=True))\n",
    "                        elif elem.name == \"ul\":\n",
    "                            items = [li.get_text(strip=True) for li in elem.find_all(\"li\")]\n",
    "                            if items:\n",
    "                                sec_desc_parts.append(\"\\n\".join(items))\n",
    "                    if sec_desc_parts:\n",
    "                        secondary_desc = \"\\n\".join(sec_desc_parts)\n",
    "                        break\n",
    "            if secondary_desc:\n",
    "                break\n",
    "    if secondary_desc:\n",
    "        data[\"Secondary_Description\"] = secondary_desc\n",
    "\n",
    "    spec_tables = soup.select(\"table.fahrzeugdaten\")\n",
    "    for table in spec_tables:\n",
    "        for row in table.select(\"tr\"):\n",
    "            label = row.select_one(\"td.label\")\n",
    "            content = row.select_one(\"td.content\")\n",
    "            if label and content:\n",
    "                key = label.get_text(strip=True).rstrip(\":\")\n",
    "                value = content.get_text(strip=True)\n",
    "                data[key] = value\n",
    "\n",
    "    price_tag = soup.select_one(\"div.sidebar-section .price span.p\")\n",
    "    if price_tag:\n",
    "        data[\"Price\"] = price_tag.get_text(strip=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_all_listings(listing_urls, max_workers=6):\n",
    "    listings_data = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(parse_listing, url): url for url in listing_urls}\n",
    "        for future in tqdm(as_completed(future_to_url), total=len(listing_urls), desc=\"Parsing listings\"):\n",
    "            listing_data = future.result()\n",
    "            listings_data.append(listing_data)\n",
    "    return pd.DataFrame(listings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 885 new listings (not in all_listings_bronze.xlsx). Parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing listings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 885/885 [01:30<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Appended 885 new listings.\n"
     ]
    }
   ],
   "source": [
    "# --- Parse new listings: Only URLs not in all_listings_bronze.xlsx ---\n",
    "excel_path = \"../data/all_listings_bronze.xlsx\"\n",
    "df_urls = pd.read_csv(\"../data/all_listing_urls.csv\")\n",
    "all_urls = set(df_urls['Listing_URL'].tolist())\n",
    "\n",
    "# Load existing data\n",
    "if os.path.exists(excel_path):\n",
    "    df_existing = pd.read_excel(excel_path)\n",
    "    df_existing.set_index(\"URL\", inplace=True)\n",
    "else:\n",
    "    df_existing = pd.DataFrame().set_index(\"URL\")\n",
    "\n",
    "existing_urls = set(df_existing.index)\n",
    "new_urls = list(all_urls - existing_urls)\n",
    "\n",
    "if new_urls:\n",
    "    print(f\"üîç Found {len(new_urls)} new listings (not in all_listings_bronze.xlsx). Parsing...\")\n",
    "    df_new = parse_all_listings(new_urls, max_workers=8)\n",
    "    df_new['Data_Hash'] = df_new.apply(lambda row: compute_hash(row, FIELDS_TO_HASH), axis=1)\n",
    "    df_new.set_index(\"URL\", inplace=True)\n",
    "    df_existing = pd.concat([df_existing, df_new])\n",
    "    print(f\"‚úÖ Appended {len(new_urls)} new listings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Checking 2794 existing listings for updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing listings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2794/2794 [15:45<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Saved changelog with 894 updates to ../changelogs\\changelog_2025-09-12-19-35-25.xlsx\n",
      "‚úÖ Updated listing data saved to '../data/all_listings_bronze.xlsx'. Total listings: 5772\n"
     ]
    }
   ],
   "source": [
    "# --- Update existing listings ---\n",
    "update_urls = list(existing_urls & all_urls)\n",
    "change_log = []\n",
    "if update_urls:\n",
    "    print(f\"üîÑ Checking {len(update_urls)} existing listings for updates...\")\n",
    "    df_updates = parse_all_listings(update_urls)\n",
    "    df_updates.set_index(\"URL\", inplace=True)\n",
    "    for url in update_urls:\n",
    "        if url in df_existing.index:\n",
    "            old_row = df_existing.loc[url].to_dict()\n",
    "            new_row = df_updates.loc[url].to_dict()\n",
    "            # --- Protection: skip update if error present ---\n",
    "            if \"Error\" in new_row and new_row[\"Error\"]:\n",
    "                print(f\"‚ö†Ô∏è Skipping update for {url} due to error: {new_row['Error']}\")\n",
    "                continue\n",
    "            old_hash = old_row.get(\"Data_Hash\", \"\")\n",
    "            new_hash = compute_hash(new_row, FIELDS_TO_HASH)\n",
    "            if old_hash != new_hash:\n",
    "                changes = get_field_changes(old_row, new_row, FIELDS_TO_HASH)\n",
    "                # --- Ignore if only Price changed and it is now 'reserved', empty, or NaN ---\n",
    "                price_changed = old_row.get(\"Price\", \"\") != new_row.get(\"Price\", \"\")\n",
    "                only_price_changed = (\n",
    "                    price_changed and\n",
    "                    all(\n",
    "                        (old_row.get(f, \"\") == new_row.get(f, \"\")) \n",
    "                        for f in FIELDS_TO_HASH if f != \"Price\"\n",
    "                    )\n",
    "                )\n",
    "                new_price = str(new_row.get(\"Price\", \"\")).strip().lower()\n",
    "                if only_price_changed and (new_price == \"reserved\" or new_price == \"\" or pd.isna(new_row.get(\"Price\", \"\"))):\n",
    "                    continue  # Skip this update\n",
    "                # --- Type compatibility fix ---\n",
    "                for col in df_existing.columns:\n",
    "                    if col in new_row:\n",
    "                        try:\n",
    "                            dtype = df_existing[col].dtype\n",
    "                            if pd.isna(new_row[col]):\n",
    "                                continue\n",
    "                            if \"datetime\" in str(dtype):\n",
    "                                new_row[col] = pd.to_datetime(new_row[col], errors=\"coerce\")\n",
    "                            elif \"float\" in str(dtype):\n",
    "                                new_row[col] = float(new_row[col])\n",
    "                            elif \"int\" in str(dtype):\n",
    "                                new_row[col] = int(float(new_row[col]))\n",
    "                            else:\n",
    "                                new_row[col] = str(new_row[col])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                new_row['Data_Hash'] = new_hash\n",
    "                df_existing.loc[url] = pd.Series(new_row)\n",
    "                change_log.append({\"URL\": url, \"Changes\": changes, \"Date\": datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")})\n",
    "    if change_log:\n",
    "        log_file = os.path.join(change_log_dir, f\"changelog_{datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}.xlsx\")\n",
    "        pd.DataFrame(change_log).to_excel(log_file, index=False)\n",
    "        print(f\"üìù Saved changelog with {len(change_log)} updates to {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated listing data saved to '../data/all_listings_bronze.xlsx'. Total listings: 6657\n"
     ]
    }
   ],
   "source": [
    "# Save the updated listings\n",
    "df_existing.reset_index(inplace=True)\n",
    "df_existing.to_excel(excel_path, index=False)\n",
    "print(f\"‚úÖ Updated listing data saved to '{excel_path}'. Total listings: {len(df_existing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating descriptions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4290/4290 [07:54<00:00,  9.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Descriptions reparsed and bronze layer updated at '../data/all_listings_bronze.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "def update_descriptions_parallel(df, max_workers=8):\n",
    "    urls = df[\"URL\"].tolist()\n",
    "    results = {}\n",
    "\n",
    "    def fetch_desc(url):\n",
    "        try:\n",
    "            parsed = parse_listing(url)\n",
    "            desc = parsed.get(\"Description\", None)\n",
    "            sec_desc = parsed.get(\"Secondary_Description\", None)\n",
    "            return url, desc, sec_desc\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update {url}: {e}\")\n",
    "            return url, None, None\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_desc, url): url for url in urls}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Updating descriptions\"):\n",
    "            url, desc, sec_desc = future.result()\n",
    "            idx = df.index[df[\"URL\"] == url]\n",
    "            if len(idx):\n",
    "                if desc is not None:\n",
    "                    df.at[idx[0], \"Description\"] = desc\n",
    "                if sec_desc is not None:\n",
    "                    df.at[idx[0], \"Secondary_Description\"] = sec_desc\n",
    "\n",
    "    return df\n",
    "\n",
    "df_existing = update_descriptions_parallel(df_existing)\n",
    "df_existing.to_excel(excel_path, index=False)\n",
    "print(f\"‚úÖ Descriptions reparsed and bronze layer updated at '{excel_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
